{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06f6130-3a48-4705-897a-455642f59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_imblearn_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import moralstrength #pls install using pip install moralstrength\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])\n",
    "# \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ad792-cd01-47b4-862f-1e1c3a7cf9d3",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdb7597-45e4-4372-8e41-45b7eaa49382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/balanced/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/balanced/test.csv\")\n",
    "\n",
    "# #indices for 10-fold cross-validation based on training data\n",
    "# splits_df = pd.read_csv(\"../data/cv_splits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75402a2-37c4-4f42-a1d2-2934077a1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10872\n",
      "Number of positive samples:  5738\n",
      "Number of negative samples:  5134\n",
      "   flag                                            content\n",
      "0     0  My bf (21) and me (22) have been together for ...\n",
      "1     0  Yesterday afternoon my(19m) aunt's boyfriend b...\n",
      "2     0  So I've been dating this girl since early dece...\n",
      "3     0  I like to send my roommate pictures of cute th...\n",
      "4     0  Good Afternoon, I'm a 29yo US Venezuelan immig...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 6000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of training samples: {}\".format(len(df_train)))\n",
    "kwargs = dict(\n",
    "    startangle = 90,\n",
    "    fontsize   = 13,\n",
    "    figsize    = (60,5),\n",
    "    autopct    = '%1.1f%%',\n",
    "    label      = ''\n",
    ")\n",
    "\n",
    "df_train['flag'].value_counts().plot.pie(**kwargs)\n",
    "\n",
    "positive_samples = df_train[df_train['flag'] == 1]\n",
    "negative_samples = df_train[df_train['flag'] == 0]\n",
    "\n",
    "print(\"Number of positive samples: \", len(positive_samples))\n",
    "print(\"Number of negative samples: \", len(negative_samples))\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9e0624-ad30-4816-b9e3-28424cf9d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 6000\n",
      "Number of positive samples:  1000\n",
      "Number of negative samples:  5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 6000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of test samples: {}\".format(len(df_test)))\n",
    "positive_samples = df_test[df_test['flag'] == 1]\n",
    "negative_samples = df_test[df_test['flag'] == 0]\n",
    "\n",
    "print(\"Number of positive samples: \", len(positive_samples))\n",
    "print(\"Number of negative samples: \", len(negative_samples))\n",
    "\n",
    "df_test['flag'].value_counts().plot.pie(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc8ab3-3d02-4633-acc0-a4fb0dae1103",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e36810-5858-4f89-b04f-33ee820487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling clitics\n",
    "def pre_normalize_sentence(text):\n",
    "    contractions = {\n",
    "        # r'[\\(\\[].*?[\\)\\]]': ' ',  # Remove all words in brackets\n",
    "        r\"(n\\'t)\": \" not\",  # Resolve contraction \"-n't\"\n",
    "        r\"(\\'ve)\": \" have\",  # Resolve contraction \"-'ve\"\n",
    "        r\"(\\'ll)\": \" will\",  # Resolve contraction \"-'ll\"\n",
    "        r\"(\\'s)\": \" is\",  # Resolve contraction \"-'s\"\n",
    "        r\"(\\'m)\": \" am\",  # Resolve contraction \"-'m\"\n",
    "        r\"(\\'d)\": \" would\",  # Resolve contraction \"-'d\"\n",
    "        r\"(\\'re)\": \" are\",  # Resolve contraction \"-'re\"\n",
    "    }\n",
    "    for pattern, replacement in contractions.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f1afb9-962a-452b-8b21-cb76581c6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(s):\n",
    "    s = s.lower()                                                   # Lowercase whole sentence\n",
    "    s = re.sub(r'\\s+', ' ', s)                                      # Remove duplicate whitespaces\n",
    "    s = re.sub(r'([.]){2,}', ' ', s)                                # Remove ellipses ...\n",
    "    s = re.sub(r'([\\w.-]+)([,;])([\\w.-]+)', '\\g<1>\\g<2> \\g<3>', s)  # Add missing whitespace after , and ;\n",
    "    s = re.sub(r'(.+)\\1{2,}', '\\g<1>\\g<1>', s)                      # Reduce repeated sequences to 2\n",
    "    s = re.sub(r'\\s+', ' ', s)                                      # Remove duplicate whitespaces (again)\n",
    "    s = s.strip()                                                   # Remove trailing whitespaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23651abb-52fb-4c84-8ef0-afb666e67508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting each post into sentences\n",
    "def read_post(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "\n",
    "    # Create a set of all unique words in both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Create vectors to represent each sentence\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Count word occurrences\n",
    "    for word in sent1:\n",
    "        vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        vector2[all_words.index(word)] += 1\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stopwords):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    # Fill the similarity matrix with cosine similarity scores\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stopwords)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def generate_summary(text, top_n=5):\n",
    "    # Load English stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = read_post(text)\n",
    "\n",
    "    # Generate the sentence similarity matrix\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Convert the similarity matrix into a graph\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "\n",
    "    # Apply the PageRank algorithm to rank the sentences\n",
    "    scores = nx.pagerank(sentence_similarity_graph, max_iter=200)\n",
    "\n",
    "    # Sort the sentences by their scores\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Extract the top sentences as the summary\n",
    "    summary = [sentence for score, sentence in ranked_sentences[:top_n]]\n",
    "\n",
    "    while len(summary) < top_n:\n",
    "      summary.append('')\n",
    "\n",
    "    return summary\n",
    "\n",
    "def split_into_sentences(post_df):\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    sent3 = []\n",
    "    sent4 = []\n",
    "    sent5 = []\n",
    "    index = 0\n",
    "        \n",
    "    for i, content in enumerate(post_df['content']):\n",
    "        array_summary = generate_summary(content)\n",
    "        sent1.append(array_summary[0])\n",
    "        sent2.append(array_summary[1])\n",
    "        sent3.append(array_summary[2])\n",
    "        sent4.append(array_summary[3])\n",
    "        sent5.append(array_summary[4])\n",
    "        index = i\n",
    "    \n",
    "    post_df['sent1'] = sent1\n",
    "    post_df['sent2'] = sent2\n",
    "    post_df['sent3'] = sent3\n",
    "    post_df['sent4'] = sent4\n",
    "    post_df['sent5'] = sent5\n",
    "\n",
    "    # To rearrange the order of sentences as specified (1.1, 1.2, ..., 1.5, then 2.1, ..., 2.5, etc.),\n",
    "    # we can first transpose the DataFrame and then use the same approach to melt it into a single column.\n",
    "    \n",
    "    # Transposing the original DataFrame so that each sentence sequence becomes a row\n",
    "    post_df_transposed = post_df[['sent1', 'sent2', 'sent3', 'sent4', 'sent5']].T\n",
    "    \n",
    "    # Resetting the index since after transposition, the original column names become the index\n",
    "    post_df_transposed.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Melting the transposed DataFrame without considering variable names, just values\n",
    "    df_long_transposed = pd.melt(post_df_transposed, value_name=\"content\")[\"content\"]\n",
    "    \n",
    "    # Converting the Series into a DataFrame\n",
    "    df_ordered = df_long_transposed.to_frame().reset_index(drop=True)\n",
    "    \n",
    "    return df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bb121e-6690-4831-b176-8127a2aeab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined preprocessing pipeline\n",
    "#pipeline using spaCy with lemmatization, stopword and punctuation removal\n",
    "def preprocess_spacy(posts, moral='No'):\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    # Assuming 'sentences' is a pandas Series or similar iterable of strings\n",
    "    for post in posts:\n",
    "        # print(post)\n",
    "        pre_normal_post = pre_normalize_sentence(post)\n",
    "        normalized_post = normalize_sentence(pre_normal_post)\n",
    "        # cleaned_post = normalized_post.replace('\\n\\n', '').replace('\\n', '')\n",
    "        doc = nlp(normalized_post)\n",
    "        if moral == 'No':\n",
    "            preprocessed_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        else:\n",
    "            preprocessed_tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "        preprocessed_data.append(preprocessed_tokens)\n",
    "\n",
    "    return preprocessed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02808b0d-55e1-4b7b-9ad8-5315a57f78a2",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baf7e4-0587-4bd4-8ce1-cfb9861c61a8",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier (processing whole text, and text split into sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b826130-c13d-4268-bacc-a73d7ed686d6",
   "metadata": {},
   "source": [
    "##### Using only handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c3ab56-c9db-4cb6-a249-d1cb926f94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1\n",
      "7248\n",
      "care column for training data has 2514 null values\n",
      "fairness column for training data has 2999 null values\n",
      "loyalty column for training data has 3498 null values\n",
      "authority column for training data has 1333 null values\n",
      "purity column for training data has 2982 null values\n",
      "       care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833   0.3125   0.833333  0.544643     0.517262\n",
      "1  0.202083  0.500000   0.5000   0.595238  0.546875     0.468839\n",
      "2  0.975000  0.895833   0.5000   0.500000  0.875000     0.749167\n",
      "3  0.500000  0.500000   0.5000   0.500000  0.500000     0.500000\n",
      "4  0.500000  0.895833   0.5000   0.833333  0.500000     0.645833\n",
      "3624\n",
      "care column for validation data has 1250 null values\n",
      "fairness column for validation data has 1483 null values\n",
      "loyalty column for validation data has 1744 null values\n",
      "authority column for validation data has 632 null values\n",
      "purity column for validation data has 1530 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "1  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "2  0.500000  0.500000  0.500000   0.642262  0.500000     0.528452\n",
      "3  0.325000  0.500000  0.500000   0.788462  0.875000     0.597692\n",
      "4  0.437500  0.895833  0.500000   0.788690  0.214286     0.567262\n",
      "Loop  2\n",
      "7248\n",
      "care column for training data has 2475 null values\n",
      "fairness column for training data has 2987 null values\n",
      "loyalty column for training data has 3495 null values\n",
      "authority column for training data has 1314 null values\n",
      "purity column for training data has 2998 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833  0.312500   0.833333  0.544643     0.517262\n",
      "1  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "2  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "3  0.500000  0.895833  0.500000   0.833333  0.500000     0.645833\n",
      "4  0.500000  0.500000  0.500000   0.642262  0.500000     0.528452\n",
      "3624\n",
      "care column for validation data has 1289 null values\n",
      "fairness column for validation data has 1495 null values\n",
      "loyalty column for validation data has 1747 null values\n",
      "authority column for validation data has 651 null values\n",
      "purity column for validation data has 1514 null values\n",
      "       care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.202083  0.500000    0.500   0.595238  0.546875     0.468839\n",
      "1  0.975000  0.895833    0.500   0.500000  0.875000     0.749167\n",
      "2  0.500000  0.500000    0.500   0.500000  0.500000     0.500000\n",
      "3  0.416667  0.454688    0.875   0.468750  0.875000     0.618021\n",
      "4  0.257440  0.895833    0.875   0.750000  0.500000     0.655655\n",
      "Loop  3\n",
      "7248\n",
      "care column for training data has 2539 null values\n",
      "fairness column for training data has 2978 null values\n",
      "loyalty column for training data has 3491 null values\n",
      "authority column for training data has 1283 null values\n",
      "purity column for training data has 3044 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.202083  0.500000  0.500000   0.595238  0.546875     0.468839\n",
      "1  0.975000  0.895833  0.500000   0.500000  0.875000     0.749167\n",
      "2  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "3  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "4  0.500000  0.500000  0.500000   0.500000  0.500000     0.500000\n",
      "3624\n",
      "care column for validation data has 1225 null values\n",
      "fairness column for validation data has 1504 null values\n",
      "loyalty column for validation data has 1751 null values\n",
      "authority column for validation data has 682 null values\n",
      "purity column for validation data has 1468 null values\n",
      "    care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.000  0.895833   0.3125   0.833333  0.544643     0.517262\n",
      "1  0.500  0.895833   0.5000   0.833333  0.500000     0.645833\n",
      "2  0.975  0.854167   0.5000   0.357143  0.544643     0.646190\n",
      "3  0.500  0.500000   0.5000   0.645833  0.875000     0.604167\n",
      "4  0.500  0.500000   0.7500   0.500000  0.833333     0.616667\n",
      "Cross-Validation F1 scores:  [0.6907645038857763, 0.6908762420957543, 0.6911127167630058]\n",
      "Average Cross-Validation F1 Score: 0.691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_nb = df_train['content'].copy()\n",
    "y_nb = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "val_f1s = []\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "    print(\"Loop \", counter)\n",
    "    X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "    y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "\n",
    "    ## Adding handcrafted features using lexicon\n",
    "    preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "    result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "    print(len(result))\n",
    "    for column in result.columns:\n",
    "        print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "        result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "        result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "    result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "    print(result.head())      \n",
    "    X_train_nb_vec = csr_matrix(result.values)\n",
    "\n",
    "    preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "    result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "    print(len(result_val))\n",
    "    # result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "    for column in result_val.columns:\n",
    "        print(f\"{column} column for validation data has {result_val[column].isnull().sum()} null values\")\n",
    "        result_val[column] = (result_val[column]-1)/8 #normalize score to 0 to 1\n",
    "        result_val[column] = result_val[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        # print(result_val[column][0:5])\n",
    "    result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "    print(result_val.head())\n",
    "    X_val_nb_vec = csr_matrix(result_val.values)\n",
    "\n",
    "    # Initialize the Bernoulli Naive Bayes model\n",
    "    model = BernoulliNB()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_nb_vec, y_train_nb)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_nb = model.predict(X_val_nb_vec)\n",
    "\n",
    "    val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "    counter+=1\n",
    "\n",
    "# Calculate and print the average F1 score across all validation folds\n",
    "# print(f\"Results for {preprocessor}:\")\n",
    "print(\"Cross-Validation F1 scores: \", val_f1s)\n",
    "print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05820783-3a61-4519-85df-954980f42976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10872\n",
      "care column for training data has 3764 null values\n",
      "fairness column for training data has 4482 null values\n",
      "loyalty column for training data has 5242 null values\n",
      "authority column for training data has 1965 null values\n",
      "purity column for training data has 4512 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833  0.312500   0.833333  0.544643     0.517262\n",
      "1  0.202083  0.500000  0.500000   0.595238  0.546875     0.468839\n",
      "2  0.975000  0.895833  0.500000   0.500000  0.875000     0.749167\n",
      "3  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "4  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "care column for test data has 0 null values\n",
      "fairness column for test data has 0 null values\n",
      "loyalty column for test data has 0 null values\n",
      "authority column for test data has 0 null values\n",
      "purity column for test data has 0 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.346667  0.500000  0.875000   0.741310  0.218750     0.536345\n",
      "1  0.333333  0.825000  0.641667   0.760417  0.423611     0.596806\n",
      "2  0.500000  0.895833  0.875000   0.833333  0.875000     0.795833\n",
      "3  0.975000  0.875000  0.875000   0.708829  0.715833     0.829933\n",
      "4  0.188294  0.895833  0.817708   0.656746  0.479464     0.607609\n",
      "Final Test F1 Score for Naive Bayes: 0.286\n"
     ]
    }
   ],
   "source": [
    "X_train_NB_handcrafted = df_train['content'].copy()\n",
    "y_train_NB_handcrafted = df_train['flag'].copy()\n",
    "X_test_NB_handcrafted = df_test['content'].copy()\n",
    "y_test_NB_handcrafted = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "## Adding handcrafted features using lexicon\n",
    "preprocessed_spacy_train = preprocess_pipeline(X_train_NB_handcrafted, moral='Yes')\n",
    "result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "print(len(result))\n",
    "for column in result.columns:\n",
    "    print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "    result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "    result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "print(result.head())\n",
    "X_train_NB_handcrafted_vec = csr_matrix(result.values)\n",
    "\n",
    "preprocessed_spacy_test = preprocess_pipeline(X_test_NB_handcrafted, moral='Yes')\n",
    "result_test = moralstrength.estimate_morals(preprocessed_spacy_test, process=False) # set to false if text is alredy pre-processed\n",
    "# result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "for column in result_test.columns:\n",
    "    print(f\"{column} column for test data has {result[column].isnull().sum()} null values\")\n",
    "    result_test[column] = (result_test[column]-1)/8 #normalize score to 0 to 1\n",
    "    result_test[column] = result_test[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "    # print(result_val[column][0:5])\n",
    "result_test['moral_score'] = result_test.mean(axis=1) #normalize score to 0 to 1\n",
    "print(result_test.head())\n",
    "X_test_NB_handcrafted_vec = csr_matrix(result_test.values)\n",
    "\n",
    "model_NB_handcrafted = BernoulliNB()\n",
    "model_NB_handcrafted.fit(X_train_NB_handcrafted_vec, y_train_NB_handcrafted)\n",
    "\n",
    "y_pred_NB_handcrafted = model_NB_handcrafted.predict(X_test_NB_handcrafted_vec)\n",
    "test_f1_score_nb_handcrafted = f1_score(y_test_NB_handcrafted, y_pred_NB_handcrafted)\n",
    "\n",
    "print(f\"Final Test F1 Score for Naive Bayes: {test_f1_score_nb_handcrafted:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253ddcf-3910-4088-b605-979792fff1dd",
   "metadata": {},
   "source": [
    "##### Naive Bayes with only td-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba85434-de07-487d-a1fe-ffbafbd8c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_nb = df_train['content'].copy()\n",
    "y_nb = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "\n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "            X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = BernoulliNB()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "\n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e7ad3d-0b1d-4b73-985c-894bed147de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10872, 29808)\n",
      "Final Test F1 Score for Naive Bayes (td-idf vectors): 0.475\n"
     ]
    }
   ],
   "source": [
    "X_train_NB_tfidf = df_train['content'].copy()\n",
    "y_train_NB_tfidf = df_train['flag'].copy()\n",
    "X_test_NB_tfidf = df_test['content'].copy()\n",
    "y_test_NB_tfidf = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_NB_tfidf = preprocess_pipeline(X_train_NB_tfidf)\n",
    "X_test_NB_tfidf = preprocess_pipeline(X_test_NB_tfidf)\n",
    "\n",
    "print(len(X_train_NB_tfidf))\n",
    "# Vectorize data\n",
    "X_train_NB_tfidf_vec = vectorizer.fit_transform(X_train_NB_tfidf)\n",
    "X_test_NB_tfidf_vec = vectorizer.transform(X_test_NB_tfidf)\n",
    "print(X_train_NB_tfidf_vec.shape)\n",
    "\n",
    "model_NB_tfidf = BernoulliNB()\n",
    "model_NB_tfidf.fit(X_train_NB_tfidf_vec, y_train_NB_tfidf)\n",
    "\n",
    "y_pred_NB_tfidf = model_NB_tfidf.predict(X_test_NB_tfidf_vec)\n",
    "test_f1_score_nb_tfidf = f1_score(y_test_NB_tfidf, y_pred_NB_tfidf)\n",
    "\n",
    "print(f\"Final Test F1 Score for Naive Bayes (td-idf vectors): {test_f1_score_nb_tfidf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814abea-4a23-421c-aca9-58c77f303d9b",
   "metadata": {},
   "source": [
    "##### Naive Bayes with both td-idf vectors and additional handcrafted features of moral_score from morality lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da27d18-5797-4ea1-9001-459a61bd3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2514 null values\n",
      "fairness column for training data has 2999 null values\n",
      "loyalty column for training data has 3498 null values\n",
      "authority column for training data has 1333 null values\n",
      "purity column for training data has 2982 null values\n",
      "3624\n",
      "care column for training data has 1250 null values\n",
      "fairness column for training data has 1483 null values\n",
      "loyalty column for training data has 1744 null values\n",
      "authority column for training data has 632 null values\n",
      "purity column for training data has 1530 null values\n",
      "Loop  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2475 null values\n",
      "fairness column for training data has 2987 null values\n",
      "loyalty column for training data has 3495 null values\n",
      "authority column for training data has 1314 null values\n",
      "purity column for training data has 2998 null values\n",
      "3624\n",
      "care column for training data has 1289 null values\n",
      "fairness column for training data has 1495 null values\n",
      "loyalty column for training data has 1747 null values\n",
      "authority column for training data has 651 null values\n",
      "purity column for training data has 1514 null values\n",
      "Loop  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2539 null values\n",
      "fairness column for training data has 2978 null values\n",
      "loyalty column for training data has 3491 null values\n",
      "authority column for training data has 1283 null values\n",
      "purity column for training data has 3044 null values\n",
      "3624\n",
      "care column for training data has 1225 null values\n",
      "fairness column for training data has 1504 null values\n",
      "loyalty column for training data has 1751 null values\n",
      "authority column for training data has 682 null values\n",
      "purity column for training data has 1468 null values\n",
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_nb = df_train['content'].copy()\n",
    "y_nb = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    counter = 1\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        print(\"Loop \", counter)\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "\n",
    "        X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "        X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "\n",
    "\n",
    "        ## Adding handcrafted features using lexicon\n",
    "        preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "        result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        # result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "        print(len(result))\n",
    "        for column in result.columns:\n",
    "            print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "            result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "            result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "        result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        X_train_nb_vec = hstack([X_train_nb_vec, result_moral_matrix])\n",
    "\n",
    "        preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "        result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        print(len(result_val))\n",
    "        for column in result_val.columns:\n",
    "            print(f\"{column} column for training data has {result_val[column].isnull().sum()} null values\")\n",
    "            result_val[column] = (result_val[column]-1)/8 #normalize score to 0 to 1\n",
    "            result_val[column] = result_val[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "        result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        X_val_nb_vec = hstack([X_val_nb_vec, result_moral_matrix_val])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = BernoulliNB()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "\n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "        counter+=1\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42cb3cc6-0d22-4259-8b4b-1ef14d0ed90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    My bf (21) and me (22) have been together for ...\n",
      "1    Yesterday afternoon my(19m) aunt's boyfriend b...\n",
      "2    So I've been dating this girl since early dece...\n",
      "3    I like to send my roommate pictures of cute th...\n",
      "4    Good Afternoon, I'm a 29yo US Venezuelan immig...\n",
      "Name: content, dtype: object\n",
      "(10872, 29808)\n",
      "10872\n",
      "care column for training data has 3764 null values\n",
      "fairness column for training data has 4482 null values\n",
      "loyalty column for training data has 5242 null values\n",
      "authority column for training data has 1965 null values\n",
      "purity column for training data has 4512 null values\n",
      "3624\n",
      "care column for training data has 1682 null values\n",
      "fairness column for training data has 2130 null values\n",
      "loyalty column for training data has 2484 null values\n",
      "authority column for training data has 811 null values\n",
      "purity column for training data has 2050 null values\n",
      "Final Test F1 Score for Naive Bayes: 0.475\n"
     ]
    }
   ],
   "source": [
    "X_train_NB_overall = df_train['content'].copy()\n",
    "y_train_NB_overall = df_train['flag'].copy()\n",
    "X_test_NB_overall = df_test['content'].copy()\n",
    "y_test_NB_overall = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_NB_overall_processed = preprocess_pipeline(X_train_NB_overall)\n",
    "X_test_NB_overall_processed = preprocess_pipeline(X_test_NB_overall)\n",
    "\n",
    "print(X_train_NB_overall.head())\n",
    "\n",
    "# Vectorize data\n",
    "X_train_NB_overall_vec = vectorizer.fit_transform(X_train_NB_overall_processed)\n",
    "X_test_NB_overall_vec = vectorizer.transform(X_test_NB_overall_processed)\n",
    "print(X_train_NB_overall_vec.shape)\n",
    "\n",
    " ## Adding handcrafted features using lexicon\n",
    "preprocessed_spacy_train = preprocess_pipeline(X_train_NB_overall, moral='Yes')\n",
    "result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "# result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "print(len(result))\n",
    "for column in result.columns:\n",
    "    print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "    result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "    result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "X_train_nb_overall_vec = hstack([X_train_NB_overall_vec, result_moral_matrix])\n",
    "\n",
    "preprocessed_spacy_val = preprocess_pipeline(X_test_NB_overall, moral='Yes')\n",
    "result_test = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "print(len(result_val))\n",
    "for column in result_test.columns:\n",
    "    print(f\"{column} column for training data has {result_test[column].isnull().sum()} null values\")\n",
    "    result_test[column] = (result_test[column]-1)/8 #normalize score to 0 to 1\n",
    "    result_test[column] = result_test[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result_test['moral_score'] = result_test.mean(axis=1) #normalize score to 0 to 1\n",
    "result_moral_matrix_test = csr_matrix((result_test['moral_score']).values.reshape(-1, 1))\n",
    "X_test_nb_overall_vec = hstack([X_test_NB_overall_vec, result_moral_matrix_test])\n",
    "\n",
    "model_NB_overall = BernoulliNB()\n",
    "model_NB_overall.fit(X_train_nb_overall_vec, y_train_NB_overall)\n",
    "\n",
    "y_pred_NB_overall = model_NB_overall.predict(X_test_nb_overall_vec)\n",
    "test_f1_score_nb_overall = f1_score(y_test_NB_overall, y_pred_NB_overall)\n",
    "\n",
    "print(f\"Final Test F1 Score for Naive Bayes: {test_f1_score_nb_overall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af5df0-70df-4afc-95a5-f2146f9d6732",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0201145-96be-4345-b084-aaa6aa8f339b",
   "metadata": {},
   "source": [
    "##### Using only handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3742c0d5-6323-4ef8-9368-dda37dc85636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1\n",
      "7248\n",
      "care column for training data has 2514 null values\n",
      "fairness column for training data has 2999 null values\n",
      "loyalty column for training data has 3498 null values\n",
      "authority column for training data has 1333 null values\n",
      "purity column for training data has 2982 null values\n",
      "       care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833   0.3125   0.833333  0.544643     0.517262\n",
      "1  0.202083  0.500000   0.5000   0.595238  0.546875     0.468839\n",
      "2  0.975000  0.895833   0.5000   0.500000  0.875000     0.749167\n",
      "3  0.500000  0.500000   0.5000   0.500000  0.500000     0.500000\n",
      "4  0.500000  0.895833   0.5000   0.833333  0.500000     0.645833\n",
      "3624\n",
      "care column for validation data has 1250 null values\n",
      "fairness column for validation data has 1483 null values\n",
      "loyalty column for validation data has 1744 null values\n",
      "authority column for validation data has 632 null values\n",
      "purity column for validation data has 1530 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "1  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "2  0.500000  0.500000  0.500000   0.642262  0.500000     0.528452\n",
      "3  0.325000  0.500000  0.500000   0.788462  0.875000     0.597692\n",
      "4  0.437500  0.895833  0.500000   0.788690  0.214286     0.567262\n",
      "Loop  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2475 null values\n",
      "fairness column for training data has 2987 null values\n",
      "loyalty column for training data has 3495 null values\n",
      "authority column for training data has 1314 null values\n",
      "purity column for training data has 2998 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833  0.312500   0.833333  0.544643     0.517262\n",
      "1  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "2  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "3  0.500000  0.895833  0.500000   0.833333  0.500000     0.645833\n",
      "4  0.500000  0.500000  0.500000   0.642262  0.500000     0.528452\n",
      "3624\n",
      "care column for validation data has 1289 null values\n",
      "fairness column for validation data has 1495 null values\n",
      "loyalty column for validation data has 1747 null values\n",
      "authority column for validation data has 651 null values\n",
      "purity column for validation data has 1514 null values\n",
      "       care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.202083  0.500000    0.500   0.595238  0.546875     0.468839\n",
      "1  0.975000  0.895833    0.500   0.500000  0.875000     0.749167\n",
      "2  0.500000  0.500000    0.500   0.500000  0.500000     0.500000\n",
      "3  0.416667  0.454688    0.875   0.468750  0.875000     0.618021\n",
      "4  0.257440  0.895833    0.875   0.750000  0.500000     0.655655\n",
      "Loop  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2539 null values\n",
      "fairness column for training data has 2978 null values\n",
      "loyalty column for training data has 3491 null values\n",
      "authority column for training data has 1283 null values\n",
      "purity column for training data has 3044 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.202083  0.500000  0.500000   0.595238  0.546875     0.468839\n",
      "1  0.975000  0.895833  0.500000   0.500000  0.875000     0.749167\n",
      "2  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "3  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "4  0.500000  0.500000  0.500000   0.500000  0.500000     0.500000\n",
      "3624\n",
      "care column for validation data has 1225 null values\n",
      "fairness column for validation data has 1504 null values\n",
      "loyalty column for validation data has 1751 null values\n",
      "authority column for validation data has 682 null values\n",
      "purity column for validation data has 1468 null values\n",
      "    care  fairness  loyalty  authority    purity  moral_score\n",
      "0  0.000  0.895833   0.3125   0.833333  0.544643     0.517262\n",
      "1  0.500  0.895833   0.5000   0.833333  0.500000     0.645833\n",
      "2  0.975  0.854167   0.5000   0.357143  0.544643     0.646190\n",
      "3  0.500  0.500000   0.5000   0.645833  0.875000     0.604167\n",
      "4  0.500  0.500000   0.7500   0.500000  0.833333     0.616667\n",
      "Cross-Validation F1 scores:  [0.6342258440046565, 0.638317541021493, 0.6387990762124711]\n",
      "Average Cross-Validation F1 Score: 0.637\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_lr = df_train['content'].copy()\n",
    "y_lr = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "val_f1s = []\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "counter = 1\n",
    "for train_index, test_index in skf.split(X_lr, y_lr):\n",
    "    print(\"Loop \", counter)\n",
    "    X_train_lr, X_val_lr = X_lr[train_index], X_lr[test_index]\n",
    "    y_train_lr, y_val_lr = y_lr[train_index], y_lr[test_index]\n",
    "\n",
    "    ## Adding handcrafted features using lexicon\n",
    "    preprocessed_spacy_train = preprocess_pipeline(X_train_lr, moral='Yes')\n",
    "    result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "    print(len(result))\n",
    "    for column in result.columns:\n",
    "        print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "        result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "        result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "    result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "    print(result.head())      \n",
    "    X_train_lr_vec = csr_matrix(result.values)\n",
    "\n",
    "    preprocessed_spacy_val = preprocess_pipeline(X_val_lr, moral='Yes')\n",
    "    result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "    print(len(result_val))\n",
    "    # result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "    for column in result_val.columns:\n",
    "        print(f\"{column} column for validation data has {result_val[column].isnull().sum()} null values\")\n",
    "        result_val[column] = (result_val[column]-1)/8 #normalize score to 0 to 1\n",
    "        result_val[column] = result_val[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        # print(result_val[column][0:5])\n",
    "    result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "    print(result_val.head())\n",
    "    X_val_lr_vec = csr_matrix(result_val.values)\n",
    "\n",
    "    # Initialize the Bernoulli Naive Bayes model\n",
    "    model = LogisticRegression(solver='saga')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_lr_vec, y_train_lr)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lr = model.predict(X_val_lr_vec)\n",
    "\n",
    "    val_f1s.append(f1_score(y_val_lr, y_pred_lr))\n",
    "    counter+=1\n",
    "\n",
    "# Calculate and print the average F1 score across all validation folds\n",
    "# print(f\"Results for {preprocessor}:\")\n",
    "print(\"Cross-Validation F1 scores: \", val_f1s)\n",
    "print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b6c3ae-aa8c-4adf-a1e1-b4e0e8756e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10872\n",
      "care column for training data has 3764 null values\n",
      "fairness column for training data has 4482 null values\n",
      "loyalty column for training data has 5242 null values\n",
      "authority column for training data has 1965 null values\n",
      "purity column for training data has 4512 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.000000  0.895833  0.312500   0.833333  0.544643     0.517262\n",
      "1  0.202083  0.500000  0.500000   0.595238  0.546875     0.468839\n",
      "2  0.975000  0.895833  0.500000   0.500000  0.875000     0.749167\n",
      "3  0.227183  0.458333  0.291667   0.500000  0.500000     0.395437\n",
      "4  0.675000  0.500000  0.760204   0.698214  0.652778     0.657239\n",
      "care column for test data has 0 null values\n",
      "fairness column for test data has 0 null values\n",
      "loyalty column for test data has 0 null values\n",
      "authority column for test data has 0 null values\n",
      "purity column for test data has 0 null values\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  0.346667  0.500000  0.875000   0.741310  0.218750     0.536345\n",
      "1  0.333333  0.825000  0.641667   0.760417  0.423611     0.596806\n",
      "2  0.500000  0.895833  0.875000   0.833333  0.875000     0.795833\n",
      "3  0.975000  0.875000  0.875000   0.708829  0.715833     0.829933\n",
      "4  0.188294  0.895833  0.817708   0.656746  0.479464     0.607609\n",
      "Final Test F1 Score for Logistic Regression (handcrafted features only): 0.304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_LR_handcrafted = df_train['content'].copy()\n",
    "y_train_LR_handcrafted = df_train['flag'].copy()\n",
    "X_test_LR_handcrafted = df_test['content'].copy()\n",
    "y_test_LR_handcrafted = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "## Adding handcrafted features using lexicon\n",
    "preprocessed_spacy_train = preprocess_pipeline(X_train_LR_handcrafted, moral='Yes')\n",
    "result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "print(len(result))\n",
    "for column in result.columns:\n",
    "    print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "    result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "    result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "print(result.head())\n",
    "X_train_LR_handcrafted_vec = csr_matrix(result.values)\n",
    "\n",
    "preprocessed_spacy_test = preprocess_pipeline(X_test_LR_handcrafted, moral='Yes')\n",
    "result_test = moralstrength.estimate_morals(preprocessed_spacy_test, process=False) # set to false if text is alredy pre-processed\n",
    "# result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "for column in result_test.columns:\n",
    "    print(f\"{column} column for test data has {result[column].isnull().sum()} null values\")\n",
    "    result_test[column] = (result_test[column]-1)/8 #normalize score to 0 to 1\n",
    "    result_test[column] = result_test[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "    # print(result_val[column][0:5])\n",
    "result_test['moral_score'] = result_test.mean(axis=1) #normalize score to 0 to 1\n",
    "print(result_test.head())\n",
    "X_test_LR_handcrafted_vec = csr_matrix(result_test.values)\n",
    "\n",
    "model_LR_handcrafted = LogisticRegression(solver='saga')\n",
    "model_LR_handcrafted.fit(X_train_LR_handcrafted_vec, y_train_LR_handcrafted)\n",
    "\n",
    "y_pred_LR_handcrafted = model_LR_handcrafted.predict(X_test_LR_handcrafted_vec)\n",
    "test_f1_score_lr_handcrafted = f1_score(y_test_LR_handcrafted, y_pred_LR_handcrafted)\n",
    "\n",
    "print(f\"Final Test F1 Score for Logistic Regression (handcrafted features only): {test_f1_score_lr_handcrafted:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b69a5-70de-494d-9d17-dd56f6fce514",
   "metadata": {},
   "source": [
    "##### Logistic Regression with only td-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b08de5-9581-43ac-b4b1-a871830f7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_lr_2 = df_train['content'].copy()\n",
    "y_lr_2 = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_lr_2, y_lr_2):\n",
    "        X_train_lr, X_val_lr = X_lr_2[train_index], X_lr_2[test_index]\n",
    "        y_train_lr, y_val_lr = y_lr_2[train_index], y_lr_2[test_index]\n",
    "\n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_lr_preprocessed = preprocess_pipeline(X_train_lr)\n",
    "            X_val_lr_preprocessed = preprocess_pipeline(X_val_lr)\n",
    "        # Vectorize data\n",
    "        X_train_lr_vec = vectorizer.fit_transform(X_train_lr_preprocessed)\n",
    "        X_val_lr_vec = vectorizer.transform(X_val_lr_preprocessed)\n",
    "\n",
    "\n",
    "        # if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "        #     y_train_nb = np.repeat(y_train_nb, 5)\n",
    "        #     y_val_nb = np.repeat(y_val_nb, 5)\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = LogisticRegression(solver='saga')\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_lr_vec, y_train_lr)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred_lr = model.predict(X_val_lr_vec)\n",
    "\n",
    "        val_f1s.append(f1_score(y_val_lr, y_pred_lr))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4f8191-804f-4d69-870f-79c8ff28953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10872, 29808)\n",
      "Final Test F1 Score for Naive Bayes (tf-idf vectors): 0.595\n"
     ]
    }
   ],
   "source": [
    "X_train_LR_tfidf = df_train['content'].copy()\n",
    "y_train_LR_tfidf = df_train['flag'].copy()\n",
    "X_test_LR_tfidf = df_test['content'].copy()\n",
    "y_test_LR_tfidf = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_LR_tfidf = preprocess_pipeline(X_train_LR_tfidf)\n",
    "X_test_LR_tfidf = preprocess_pipeline(X_test_LR_tfidf)\n",
    "\n",
    "print(len(X_train_LR_tfidf))\n",
    "# Vectorize data\n",
    "X_train_LR_tfidf_vec = vectorizer.fit_transform(X_train_LR_tfidf)\n",
    "X_test_LR_tfidf_vec = vectorizer.transform(X_test_LR_tfidf)\n",
    "print(X_train_LR_tfidf_vec.shape)\n",
    "\n",
    "model_LR_tfidf = LogisticRegression(solver='saga')\n",
    "model_LR_tfidf.fit(X_train_LR_tfidf_vec, y_train_LR_tfidf)\n",
    "\n",
    "y_pred_LR_tfidf = model_LR_tfidf.predict(X_test_LR_tfidf_vec)\n",
    "test_f1_score_lr_tfidf = f1_score(y_test_LR_tfidf, y_pred_LR_tfidf)\n",
    "\n",
    "print(f\"Final Test F1 Score for Naive Bayes (tf-idf vectors): {test_f1_score_lr_tfidf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d06ddc-c8e2-4cb9-865e-57a5406a7f8c",
   "metadata": {},
   "source": [
    "##### Logistic Regression with both td-idf vectors and additional handcrafted features of moral_score from morality lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af01e816-ed05-4bd5-96cf-0159497de29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2514 null values\n",
      "fairness column for training data has 2999 null values\n",
      "loyalty column for training data has 3498 null values\n",
      "authority column for training data has 1333 null values\n",
      "purity column for training data has 2982 null values\n",
      "3624\n",
      "care column for training data has 1250 null values\n",
      "fairness column for training data has 1483 null values\n",
      "loyalty column for training data has 1744 null values\n",
      "authority column for training data has 632 null values\n",
      "purity column for training data has 1530 null values\n",
      "Loop  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2475 null values\n",
      "fairness column for training data has 2987 null values\n",
      "loyalty column for training data has 3495 null values\n",
      "authority column for training data has 1314 null values\n",
      "purity column for training data has 2998 null values\n",
      "3624\n",
      "care column for training data has 1289 null values\n",
      "fairness column for training data has 1495 null values\n",
      "loyalty column for training data has 1747 null values\n",
      "authority column for training data has 651 null values\n",
      "purity column for training data has 1514 null values\n",
      "Loop  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248\n",
      "care column for training data has 2539 null values\n",
      "fairness column for training data has 2978 null values\n",
      "loyalty column for training data has 3491 null values\n",
      "authority column for training data has 1283 null values\n",
      "purity column for training data has 3044 null values\n",
      "3624\n",
      "care column for training data has 1225 null values\n",
      "fairness column for training data has 1504 null values\n",
      "loyalty column for training data has 1751 null values\n",
      "authority column for training data has 682 null values\n",
      "purity column for training data has 1468 null values\n",
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_lr_3 = df_train['content'].copy()\n",
    "y_lr_3 = df_train['flag'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 3  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    counter = 1\n",
    "    for train_index, test_index in skf.split(X_lr_3, y_lr_3):\n",
    "        print(\"Loop \", counter)\n",
    "        X_train_lr, X_val_lr = X_lr_3[train_index], X_lr_3[test_index]\n",
    "        y_train_lr, y_val_lr = y_lr_3[train_index], y_lr_3[test_index]\n",
    "\n",
    "        X_train_lr_preprocessed = preprocess_pipeline(X_train_lr)\n",
    "        X_val_lr_preprocessed = preprocess_pipeline(X_val_lr)\n",
    "\n",
    "        # Vectorize data\n",
    "        X_train_lr_vec = vectorizer.fit_transform(X_train_lr_preprocessed)\n",
    "        X_val_lr_vec = vectorizer.transform(X_val_lr_preprocessed)\n",
    "\n",
    "        # if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "        #     y_train_nb = np.repeat(y_train_nb, 5)\n",
    "        #     y_val_nb = np.repeat(y_val_nb, 5)\n",
    "        #     # print(y_train_nb.shape)\n",
    "        #     # print(y_train_nb[0:10])\n",
    "\n",
    "        ## Adding handcrafted features using lexicon\n",
    "        preprocessed_spacy_train = preprocess_pipeline(X_train_lr, moral='Yes')\n",
    "        result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        # result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "        print(len(result))\n",
    "        for column in result.columns:\n",
    "            print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "            result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "            result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "        result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        X_train_lr_vec = hstack([X_train_lr_vec, result_moral_matrix])\n",
    "\n",
    "        preprocessed_spacy_val = preprocess_pipeline(X_val_lr, moral='Yes')\n",
    "        result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        print(len(result_val))\n",
    "        for column in result_val.columns:\n",
    "            print(f\"{column} column for training data has {result_val[column].isnull().sum()} null values\")\n",
    "            result_val[column] = (result_val[column]-1)/8 #normalize score to 0 to 1\n",
    "            result_val[column] = result_val[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "        result_val['moral_score'] = result_val.mean(axis=1) #normalize score to 0 to 1\n",
    "        result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        X_val_lr_vec = hstack([X_val_lr_vec, result_moral_matrix_val])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = LogisticRegression(solver='saga')\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train_lr_vec, y_train_lr)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred_lr = model.predict(X_val_lr_vec)\n",
    "\n",
    "        val_f1s.append(f1_score(y_val_lr, y_pred_lr))\n",
    "        counter+=1\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f3d91b-8598-49ba-8e42-f84317e89b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10872, 29808)\n",
      "10872\n",
      "care column for training data has 3764 null values\n",
      "fairness column for training data has 4482 null values\n",
      "loyalty column for training data has 5242 null values\n",
      "authority column for training data has 1965 null values\n",
      "purity column for training data has 4512 null values\n",
      "3624\n",
      "care column for training data has 1682 null values\n",
      "fairness column for training data has 2130 null values\n",
      "loyalty column for training data has 2484 null values\n",
      "authority column for training data has 811 null values\n",
      "purity column for training data has 2050 null values\n",
      "Final Test F1 Score for Logistic Regression (combined): 0.594\n"
     ]
    }
   ],
   "source": [
    "X_train_LR_overall = df_train['content'].copy()\n",
    "y_train_LR_overall = df_train['flag'].copy()\n",
    "X_test_LR_overall = df_test['content'].copy()\n",
    "y_test_LR_overall = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_LR_overall_preprocessed = preprocess_pipeline(X_train_LR_overall)\n",
    "X_test_LR_overall_preprocessed = preprocess_pipeline(X_test_LR_overall)\n",
    "\n",
    "# Vectorize data\n",
    "X_train_LR_overall_vec = vectorizer.fit_transform(X_train_LR_overall_preprocessed)\n",
    "X_test_LR_overall_vec = vectorizer.transform(X_test_LR_overall_preprocessed)\n",
    "print(X_train_LR_overall_vec.shape)\n",
    "\n",
    " ## Adding handcrafted features using lexicon\n",
    "preprocessed_spacy_train = preprocess_pipeline(X_train_LR_overall, moral='Yes')\n",
    "result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "# result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "print(len(result))\n",
    "for column in result.columns:\n",
    "    print(f\"{column} column for training data has {result[column].isnull().sum()} null values\")\n",
    "    result[column] = (result[column]-1)/8 #normalize score to 0 to 1\n",
    "    result[column] = result[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result['moral_score'] = result.mean(axis=1) #normalize score to 0 to 1\n",
    "result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "X_train_lr_overall_vec = hstack([X_train_LR_overall_vec, result_moral_matrix])\n",
    "\n",
    "preprocessed_spacy_val = preprocess_pipeline(X_test_LR_overall, moral='Yes')\n",
    "result_test = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "print(len(result_val))\n",
    "for column in result_test.columns:\n",
    "    print(f\"{column} column for training data has {result_test[column].isnull().sum()} null values\")\n",
    "    result_test[column] = (result_test[column]-1)/8 #normalize score to 0 to 1\n",
    "    result_test[column] = result_test[column].fillna(0.5) #assign moral score of -1 to null values\n",
    "result_test['moral_score'] = result_test.mean(axis=1) #normalize score to 0 to 1\n",
    "result_moral_matrix_test = csr_matrix((result_test['moral_score']).values.reshape(-1, 1))\n",
    "X_test_lr_overall_vec = hstack([X_test_LR_overall_vec, result_moral_matrix_test])\n",
    "\n",
    "model_LR_overall = LogisticRegression(solver='saga')\n",
    "model_LR_overall.fit(X_train_lr_overall_vec, y_train_LR_overall)\n",
    "\n",
    "y_pred_LR_overall = model_LR_overall.predict(X_test_lr_overall_vec)\n",
    "test_f1_score_lr_overall = f1_score(y_test_LR_overall, y_pred_LR_overall)\n",
    "\n",
    "print(f\"Final Test F1 Score for Logistic Regression (combined): {test_f1_score_lr_overall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70006635-394c-42b1-a033-3b11db05043a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f0be6-68d6-4fa4-a72b-611331a233e6",
   "metadata": {},
   "source": [
    "### Logistic Regression (tf-idf vectors only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea6bed1-21b7-4301-aa8f-4cbf17605f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.53138585 0.5313035  0.53130443 0.71750356 0.71729468 0.71690481\n",
      "        nan        nan        nan 0.5245239  0.52402589 0.52420826\n",
      " 0.70252568 0.70237655 0.70195873        nan        nan        nan\n",
      " 0.6411223  0.63806379 0.63786996 0.70451869 0.70626418 0.70597056\n",
      "        nan        nan        nan 0.63527087 0.63589351 0.63484146\n",
      " 0.69621615 0.69739276 0.69682602        nan        nan        nan\n",
      " 0.6843707  0.68157583 0.68264998 0.68958375 0.69261441 0.69054887\n",
      "        nan        nan        nan 0.67764337 0.67680632 0.67742016\n",
      " 0.68546636 0.68688454 0.68506056        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model train f1 score: 0.717503558426043\n",
      "Best hyperparameters: {'logistic__C': 0.1, 'logistic__fit_intercept': True, 'logistic__penalty': 'l2', 'logistic__solver': 'saga', 'tfidf__max_features': 5000}\n",
      "Best model test F1 score: 0.4976680384087791\n"
     ]
    }
   ],
   "source": [
    "#initialise copies of train-test split for use with classifier\n",
    "X_train_LR_tuning = df_train['content'].copy()\n",
    "y_train_LR_tuning = df_train['flag'].copy()\n",
    "X_test_LR_tuning = df_test['content'].copy()\n",
    "y_test_LR_tuning = df_test['flag'].copy()\n",
    "\n",
    "#using preprocess_pipeline as vectorizer\n",
    "preprocess_pipeline = preprocess_spacy\n",
    "# vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_LR_tuning_preprocessed = preprocess_pipeline(X_train_LR_tuning)\n",
    "X_test_LR_tuning_preprocessed = preprocess_pipeline(X_test_LR_tuning)\n",
    "\n",
    "# Set up a pipeline with a vectorizer and logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True)),\n",
    "    ('logistic', LogisticRegression(solver='saga'))\n",
    "])\n",
    "\n",
    "# Define a parameter grid to search over for Naive Bayes\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 6000, 7000],\n",
    "    'logistic__C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'logistic__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'logistic__fit_intercept': (True, False),\n",
    "    'logistic__solver': ['saga'] #('newton-cg', 'sag', 'saga')\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV \n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10, verbose=1, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# Fit the grid search \n",
    "grid_search.fit(X_train_LR_tuning_preprocessed, y_train_LR_tuning)\n",
    "\n",
    "# Best parameters and score \n",
    "print(\"Best model train f1 score:\", grid_search.best_score_)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_LR_tuning_preprocessed)\n",
    "test_f1 = f1_score(y_test_LR_tuning, y_pred_best)\n",
    "\n",
    "# test_f1 = best_model.score(X_test_LR_tuning_preprocessed, y_test_LR_tuning)\n",
    "print(\"Best model test F1 score:\", test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351bd59-ed24-4475-9a0d-b87795c98e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
