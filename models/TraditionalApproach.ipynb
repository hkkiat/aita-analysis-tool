{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f06f6130-3a48-4705-897a-455642f59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_imblearn_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import moralstrength #pls install using pip install moralstrength\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c51078b8-916a-43fe-8896-2d654498fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/huikhangkiat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords.discard('not')  \n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ad792-cd01-47b4-862f-1e1c3a7cf9d3",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebdb7597-45e4-4372-8e41-45b7eaa49382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/df_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/df_test.csv\")\n",
    "\n",
    "#indices for 10-fold cross-validation based on training data\n",
    "splits_df = pd.read_csv(\"../data/cv_splits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f75402a2-37c4-4f42-a1d2-2934077a1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2852\n",
      "original_index       0\n",
      "content              0\n",
      "label             1426\n",
      "dtype: int64\n",
      "original_index       0\n",
      "content              0\n",
      "label             1426\n",
      "dtype: int64\n",
      "   original_index                                            content  label\n",
      "0            1804  I've been dating (G) for 5 years, and was cons...      1\n",
      "1            6394  Me (36F) and my Bestie (24F) (let’s call her T...      0\n",
      "2            3406  My boyfriend of 5 years (m26) and I (f25) got ...      0\n",
      "3            4471  I was told to come here. My son was married to...      0\n",
      "4            2000  I (45F) only use Reddit for the cats so bear w...      1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 6000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of training samples: {}\".format(len(df_train)))\n",
    "kwargs = dict(\n",
    "    startangle = 90,\n",
    "    fontsize   = 13,\n",
    "    figsize    = (60,5),\n",
    "    autopct    = '%1.1f%%',\n",
    "    label      = ''\n",
    ")\n",
    "\n",
    "df_train['label'].value_counts().plot.pie(**kwargs)\n",
    "\n",
    "#Undersampling majority class to balance dataset\n",
    "majority_df = df_train[df_train['label'] == 0]\n",
    "minority_df = df_train[df_train['label'] == 1] \n",
    "downsampled_majority_df = resample(majority_df, replace=False, n_samples=1426)\n",
    "df_train = pd.concat([downsampled_majority_df, minority_df], ignore_index=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df_train[df_train == 1].count())\n",
    "print(df_train[df_train == 0].count())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9e0624-ad30-4816-b9e3-28424cf9d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 2385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 6000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of test samples: {}\".format(len(df_test)))\n",
    "df_test['label'].value_counts().plot.pie(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc8ab3-3d02-4633-acc0-a4fb0dae1103",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8e36810-5858-4f89-b04f-33ee820487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling clitics\n",
    "def pre_normalize_sentence(text):\n",
    "    contractions = {\n",
    "        # r'[\\(\\[].*?[\\)\\]]': ' ',  # Remove all words in brackets\n",
    "        r\"(n\\'t)\": \" not\",  # Resolve contraction \"-n't\"\n",
    "        r\"(\\'ve)\": \" have\",  # Resolve contraction \"-'ve\"\n",
    "        r\"(\\'ll)\": \" will\",  # Resolve contraction \"-'ll\"\n",
    "        r\"(\\'s)\": \" is\",  # Resolve contraction \"-'s\"\n",
    "        r\"(\\'m)\": \" am\",  # Resolve contraction \"-'m\"\n",
    "        r\"(\\'d)\": \" would\",  # Resolve contraction \"-'d\"\n",
    "        r\"(\\'re)\": \" are\",  # Resolve contraction \"-'re\"\n",
    "    }\n",
    "    for pattern, replacement in contractions.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da572298-6338-413e-a55b-809a94814df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_token_list(token_list):\n",
    "    for j, token in enumerate(token_list):\n",
    "        text = token\n",
    "        text = text.lower()                                                           # case folding\n",
    "        text = re.sub(r'\\s+', ' ', text)                                              # remove duplicate white space\n",
    "        text = re.sub(r'([.]){2,}', ' ', text)  # Correctly remove ellipses\n",
    "        text = re.sub(r'([\\w.-]+)([,;])([\\w.-]+)', '\\g<1>\\g<2> \\g<3>', text)          # Add missing whitespace after , and ;\n",
    "        text = re.sub(r'\\s+', ' ', text)                                              # remove duplicate white space again\n",
    "        text = text.strip()\n",
    "        token_list[j] = text                                                          # Remove trailing whitespaces\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30106462-fa5e-4534-9a91-e0fabb4d88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword and punctuation removal\n",
    "extended_punctuation = string.punctuation + '‘’“”'\n",
    "\n",
    "def remove_stopwords_punctuation(token_list):\n",
    "    return [token for token in token_list if token not in nltk_stopwords and not all(char in extended_punctuation for char in token)]\n",
    "    # return [token for token in token_list if token not in nltk_stopwords and token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b853191-a56e-4825-ab4f-197a5eaf0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_tokens(token_list, stemmer=porter_stemmer):\n",
    "    return [stemmer.stem(token) for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a616f05-3889-4293-b3f7-ff583c3fa663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "def pos_tagging(token_list):\n",
    "    return pos_tag(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2517859c-d96d-41b2-9c70-82ead168d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatisation\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemma_tokens(pos_tag_list):\n",
    "    lemma_list = []\n",
    "    for token, tag in pos_tag_list:\n",
    "        word_type = 'n' # Default if all fails\n",
    "        tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"v\"\n",
    "        if tag_simple in ['n', 'v', 'r']:\n",
    "            # If the POS tag starts with \"n\",\"v\", or \"r\", we know it's a noun, verb, or adverb\n",
    "            word_type = tag_simple \n",
    "        elif tag_simple in ['j']:\n",
    "            # If the POS tag starts with a \"j\", we know it's an adjective\n",
    "            word_type = 'a' \n",
    "        lemma_list.append(wordnet_lemmatizer.lemmatize(token.lower(), pos=word_type))\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "23651abb-52fb-4c84-8ef0-afb666e67508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting each post into sentences\n",
    "def read_post(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    # # Tokenize and remove stopwords\n",
    "    # if stopwords is None:\n",
    "    #     stopwords = []\n",
    "    # sent1 = [word.lower() for word in word_tokenize(sent1) if word.lower() not in stopwords]\n",
    "    # sent2 = [word.lower() for word in word_tokenize(sent2) if word.lower() not in stopwords]\n",
    "\n",
    "    # Create a set of all unique words in both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Create vectors to represent each sentence\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Count word occurrences\n",
    "    for word in sent1:\n",
    "        vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        vector2[all_words.index(word)] += 1\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stopwords):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    # Fill the similarity matrix with cosine similarity scores\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stopwords)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def generate_summary(text, top_n=5):\n",
    "    # Load English stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = read_post(text)\n",
    "\n",
    "    # Generate the sentence similarity matrix\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Convert the similarity matrix into a graph\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "\n",
    "    # Apply the PageRank algorithm to rank the sentences\n",
    "    scores = nx.pagerank(sentence_similarity_graph, max_iter=200)\n",
    "\n",
    "    # Sort the sentences by their scores\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Extract the top sentences as the summary\n",
    "    summary = [sentence for score, sentence in ranked_sentences[:top_n]]\n",
    "\n",
    "    while len(summary) < top_n:\n",
    "      summary.append('')\n",
    "\n",
    "    return summary\n",
    "\n",
    "def split_into_sentences(post_df):\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    sent3 = []\n",
    "    sent4 = []\n",
    "    sent5 = []\n",
    "    index = 0\n",
    "        \n",
    "    for i, content in enumerate(post_df['content']):\n",
    "        array_summary = generate_summary(content)\n",
    "        sent1.append(array_summary[0])\n",
    "        sent2.append(array_summary[1])\n",
    "        sent3.append(array_summary[2])\n",
    "        sent4.append(array_summary[3])\n",
    "        sent5.append(array_summary[4])\n",
    "        index = i\n",
    "    \n",
    "    post_df['sent1'] = sent1\n",
    "    post_df['sent2'] = sent2\n",
    "    post_df['sent3'] = sent3\n",
    "    post_df['sent4'] = sent4\n",
    "    post_df['sent5'] = sent5\n",
    "\n",
    "    # To rearrange the order of sentences as specified (1.1, 1.2, ..., 1.5, then 2.1, ..., 2.5, etc.),\n",
    "    # we can first transpose the DataFrame and then use the same approach to melt it into a single column.\n",
    "    \n",
    "    # Transposing the original DataFrame so that each sentence sequence becomes a row\n",
    "    post_df_transposed = post_df[['sent1', 'sent2', 'sent3', 'sent4', 'sent5']].T\n",
    "    \n",
    "    # Resetting the index since after transposition, the original column names become the index\n",
    "    post_df_transposed.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Melting the transposed DataFrame without considering variable names, just values\n",
    "    df_long_transposed = pd.melt(post_df_transposed, value_name=\"content\")[\"content\"]\n",
    "    \n",
    "    # Converting the Series into a DataFrame\n",
    "    df_ordered = df_long_transposed.to_frame().reset_index(drop=True)\n",
    "    \n",
    "    return df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "10bb121e-6690-4831-b176-8127a2aeab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined preprocessing pipeline\n",
    "# pipeline with nltk + lemmatization\n",
    "def preprocess_pipeline_lemma(sentences):\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        normalized_sentence = pre_normalize_sentence(sentence)\n",
    "        tweet_tokenizer = TweetTokenizer()\n",
    "        tokens = tweet_tokenizer.tokenize(normalized_sentence)\n",
    "        tokens = normalized_token_list(tokens)\n",
    "        tokens = remove_stopwords_punctuation(tokens)\n",
    "        tokens = pos_tagging(tokens)\n",
    "        tokens = lemma_tokens(tokens)\n",
    "        #removing empty records after preprocessing\n",
    "        # if len(tokens) == 0:\n",
    "        #     print(sentences.iloc[idx])\n",
    "        #     continue\n",
    "        preprocessed_data.append(tokens)\n",
    "        # if len(labels) > 0:\n",
    "        #     preprocessed_labels.append(labels.iloc[idx])\n",
    "    return preprocessed_data\n",
    "\n",
    "# pipeline with nltk + stemming\n",
    "def preprocess_pipeline_stem(sentences, stemmer=porter_stemmer):\n",
    "    preprocessed_data = []\n",
    "    # preprocessed_labels = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        normalized_sentence = pre_normalize_sentence(sentence)\n",
    "        tweet_tokenizer = TweetTokenizer()\n",
    "        tokens = tweet_tokenizer.tokenize(normalized_sentence)\n",
    "        tokens = normalized_token_list(tokens)\n",
    "        tokens = remove_stopwords_punctuation(tokens)\n",
    "        tokens = stem_tokens(tokens, stemmer)\n",
    "        #removing empty records after preprocessing\n",
    "        # if len(tokens) == 0:\n",
    "        #     print(sentences.iloc[idx])\n",
    "        #     continue\n",
    "        preprocessed_data.append(tokens)\n",
    "        # if len(labels) > 0:\n",
    "        #     preprocessed_labels.append(labels.iloc[idx])\n",
    "    return preprocessed_data\n",
    "\n",
    "#pipeline using spaCy with lemmatization, stopword and punctuation removal\n",
    "def preprocess_spacy(posts, moral='No'):\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    # Assuming 'sentences' is a pandas Series or similar iterable of strings\n",
    "    for post in posts:\n",
    "        # print(post)\n",
    "        normalized_post = pre_normalize_sentence(post)\n",
    "        cleaned_post = normalized_post.replace('\\n\\n', '').replace('\\n', '')\n",
    "        doc = nlp(cleaned_post)\n",
    "        if moral == 'No':\n",
    "            preprocessed_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        else:\n",
    "            preprocessed_tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "        preprocessed_data.append(preprocessed_tokens)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "#pipeline using spaCy with lemmatization, stopword and punctuation removal\n",
    "def preprocess_spacy_moral_sentences(posts, moral='No'):\n",
    "    posts = posts.to_frame(name='content')\n",
    "    posts = split_into_sentences(posts)\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    # Assuming 'sentences' is a pandas Series or similar iterable of strings\n",
    "    for post in posts['content']:\n",
    "        normalized_post = pre_normalize_sentence(post)\n",
    "        cleaned_post = normalized_post.replace('\\n\\n', '').replace('\\n', '')\n",
    "        doc = nlp(cleaned_post)\n",
    "        if moral == 'No':\n",
    "            preprocessed_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        else:\n",
    "            preprocessed_tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "        preprocessed_data.append(preprocessed_tokens)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32e6c9f1-58e2-4e42-8e84-e5d36196fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          care  fairness   loyalty authority    purity\n",
      "0          1.6       NaN       NaN  7.777778       NaN\n",
      "1      5.02381       8.0  7.781746  5.285714       8.0\n",
      "2     8.166667       NaN       8.0    7.4375       NaN\n",
      "3     2.285714       NaN  4.785714      6.58       8.0\n",
      "4          NaN  7.683333       6.5     7.325  2.666667\n",
      "...        ...       ...       ...       ...       ...\n",
      "2847       NaN       NaN  8.291667  7.389881      2.75\n",
      "2848       NaN       NaN       NaN  3.857143       3.0\n",
      "2849       NaN  6.940476  7.541667  6.888889       NaN\n",
      "2850       NaN       NaN       8.0  6.791667  2.714286\n",
      "2851       4.7  5.588889       7.5  7.480072  7.095238\n",
      "\n",
      "[2852 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_spacy_train = preprocess_spacy(df_train['content'], morals='Yes')\n",
    "result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7a18390-7640-45bc-9686-a2b0f3f574d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "       care  fairness   loyalty  authority    purity  moral_score\n",
      "0  2.285714       NaN  4.785714   6.580000  8.000000     4.438543\n",
      "1       NaN  7.683333  6.500000   7.325000  2.666667     4.955875\n",
      "2       NaN  7.000000  8.000000   7.294444  6.000000     5.800361\n",
      "3  4.630952  7.200000  7.928571   3.857143  6.428571     5.107690\n",
      "4       NaN  8.166667  8.000000        NaN  7.666667     6.156944\n",
      "0\n",
      "1426\n"
     ]
    }
   ],
   "source": [
    "result['moral_score'] = result.mean(axis=1)\n",
    "print(result.head())\n",
    "print((result['moral_score'].isnull.sum())\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02808b0d-55e1-4b7b-9ad8-5315a57f78a2",
   "metadata": {},
   "source": [
    "## Classificaiton Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baf7e4-0587-4bd4-8ce1-cfb9861c61a8",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier (processing whole text, and text split into sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814abea-4a23-421c-aca9-58c77f303d9b",
   "metadata": {},
   "source": [
    "##### Including additional handcrafted feature of moral_score from morality lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5da27d18-5797-4ea1-9001-459a61bd3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:55: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.610\n",
      "\n",
      "spacy_sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:55: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:55: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_sentences:\n",
      "Average Cross-Validation F1 Score: 0.536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/947342541.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n"
     ]
    }
   ],
   "source": [
    "X_nb = df_train['content'].copy()\n",
    "y_nb = df_train['label'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 2  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    # 'lemmatization_tfidf': (preprocess_pipeline_lemma, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True)),\n",
    "    # 'stemming_tfidf': (preprocess_pipeline_stem, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'vanilla_tfidf': (None, TfidfVectorizer(ngram_range=(1,1), sublinear_tf=True)),\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'lemmatization_bow': (preprocess_pipeline_lemma, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'stemming_bow': (preprocess_pipeline_stem, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'vanilla_bow': (None, CountVectorizer(ngram_range=(1,1))),\n",
    "    # 'spacy_bow': (preprocess_spacy, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "            X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "            # print(X_train_nb_preprocessed[0:10])\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "\n",
    "        if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "            y_train_nb = np.repeat(y_train_nb, 5)\n",
    "            y_val_nb = np.repeat(y_val_nb, 5)\n",
    "            # print(y_train_nb.shape)\n",
    "            # print(y_train_nb[0:10])\n",
    "        \n",
    "        \n",
    "        # # Add morality score and append it to X_train_nb_vec and X_test_nb_vec\n",
    "        # X_train_nb = X_train_nb.to_frame()\n",
    "        # X_test_nb = X_test_nb.to_frame()\n",
    "\n",
    "        # print(type(X_train_nb_vec))\n",
    "        preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "        result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        result['moral_score'] = result.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        result['moral_score'] = result['moral_score'].fillna(-1)\n",
    "        result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        X_train_nb_vec = hstack([X_train_nb_vec, result_moral_matrix])\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        \n",
    "        preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "        result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        #normalizing moral score (scale of 0-10, normalized to 0-1)\n",
    "        result_val['moral_score'] = result_val.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
    "        result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        X_val_nb_vec = hstack([X_val_nb_vec, result_moral_matrix_val])\n",
    "\n",
    "        # # Dropping rows with null value for moral score\n",
    "        keep_indices_train = result[result['moral_score'] != -1].index\n",
    "        keep_indices_val = result_val[result_val['moral_score'] != -1].index\n",
    "        # Filter the stacked matrices to keep only the desired rows\n",
    "        X_train_nb_vec = X_train_nb_vec[keep_indices_train, :]\n",
    "        X_val_nb_vec = X_val_nb_vec[keep_indices_val, :]\n",
    "        y_train_nb = y_train_nb.iloc[keep_indices_train]\n",
    "        y_val_nb = y_val_nb.iloc[keep_indices_val]\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "        # # Normalize the training and validation vectors\n",
    "        # normalizer = Normalizer()\n",
    "        # X_train_nb_vec_normalized = normalizer.fit_transform(X_train_nb_vec)\n",
    "        # X_val_nb_vec_normalized = normalizer.transform(X_val_nb_vec)\n",
    "\n",
    "        # print(X_train_nb_vec_normalized[0])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = BernoulliNB()\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "    \n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "        \n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253ddcf-3910-4088-b605-979792fff1dd",
   "metadata": {},
   "source": [
    "##### No additional handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bba85434-de07-487d-a1fe-ffbafbd8c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.612\n",
      "\n",
      "spacy_sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_sentences:\n",
      "Average Cross-Validation F1 Score: 0.533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_nb = df_train['content'].copy()\n",
    "y_nb = df_train['label'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 2  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    # 'lemmatization_tfidf': (preprocess_pipeline_lemma, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True)),\n",
    "    # 'stemming_tfidf': (preprocess_pipeline_stem, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'vanilla_tfidf': (None, TfidfVectorizer(ngram_range=(1,1), sublinear_tf=True)),\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'lemmatization_bow': (preprocess_pipeline_lemma, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'stemming_bow': (preprocess_pipeline_stem, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'vanilla_bow': (None, CountVectorizer(ngram_range=(1,1))),\n",
    "    # 'spacy_bow': (preprocess_spacy, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "            X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "            # print(X_train_nb_preprocessed[0:10])\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "\n",
    "        if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "            y_train_nb = np.repeat(y_train_nb, 5)\n",
    "            y_val_nb = np.repeat(y_val_nb, 5)\n",
    "            # print(y_train_nb.shape)\n",
    "            # print(y_train_nb[0:10])\n",
    "        \n",
    "        \n",
    "        # # Add morality score and append it to X_train_nb_vec and X_test_nb_vec\n",
    "        # X_train_nb = X_train_nb.to_frame()\n",
    "        # X_test_nb = X_test_nb.to_frame()\n",
    "\n",
    "        # # print(type(X_train_nb_vec))\n",
    "        # preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "        # result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        # result['moral_score'] = result.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        # result['moral_score'] = result['moral_score'].fillna(-1)\n",
    "        # result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        # X_train_nb_vec = hstack([X_train_nb_vec, result_moral_matrix])\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        \n",
    "        # preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "        # result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        # #normalizing moral score (scale of 0-10, normalized to 0-1)\n",
    "        # result_val['moral_score'] = result_val.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        # result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
    "        # result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        # X_val_nb_vec = hstack([X_val_nb_vec, result_moral_matrix_val])\n",
    "\n",
    "        # # # Dropping rows with null value for moral score\n",
    "        # keep_indices_train = result[result['moral_score'] != -1].index\n",
    "        # keep_indices_val = result_val[result_val['moral_score'] != -1].index\n",
    "        # # Filter the stacked matrices to keep only the desired rows\n",
    "        # X_train_nb_vec = X_train_nb_vec[keep_indices_train, :]\n",
    "        # X_val_nb_vec = X_val_nb_vec[keep_indices_val, :]\n",
    "        # y_train_nb = y_train_nb.iloc[keep_indices_train]\n",
    "        # y_val_nb = y_val_nb.iloc[keep_indices_val]\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "        # # # Normalize the training and validation vectors\n",
    "        # # normalizer = Normalizer()\n",
    "        # # X_train_nb_vec_normalized = normalizer.fit_transform(X_train_nb_vec)\n",
    "        # # X_val_nb_vec_normalized = normalizer.transform(X_val_nb_vec)\n",
    "\n",
    "        # # print(X_train_nb_vec_normalized[0])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = BernoulliNB()\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "    \n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "        \n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af5df0-70df-4afc-95a5-f2146f9d6732",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d06ddc-c8e2-4cb9-865e-57a5406a7f8c",
   "metadata": {},
   "source": [
    "##### With additional handcrafted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "af01e816-ed05-4bd5-96cf-0159497de29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.581\n",
      "\n",
      "spacy_sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result['moral_score'] = result['moral_score'].fillna(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_sentences:\n",
      "Average Cross-Validation F1 Score: 0.530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/x8qlyms56rsb_h37mylbhlz80000gn/T/ipykernel_81934/428183430.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n"
     ]
    }
   ],
   "source": [
    "X_lr = df_train['content'].copy()\n",
    "y_lr = df_train['label'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 2  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    # 'lemmatization_tfidf': (preprocess_pipeline_lemma, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True)),\n",
    "    # 'stemming_tfidf': (preprocess_pipeline_stem, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'vanilla_tfidf': (None, TfidfVectorizer(ngram_range=(1,1), sublinear_tf=True)),\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'lemmatization_bow': (preprocess_pipeline_lemma, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'stemming_bow': (preprocess_pipeline_stem, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'vanilla_bow': (None, CountVectorizer(ngram_range=(1,1))),\n",
    "    # 'spacy_bow': (preprocess_spacy, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "            X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "\n",
    "        if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "            y_train_nb = np.repeat(y_train_nb, 5)\n",
    "            y_val_nb = np.repeat(y_val_nb, 5)\n",
    "            # print(y_train_nb.shape)\n",
    "            # print(y_train_nb[0:10])\n",
    "        \n",
    "        \n",
    "        # # Add morality score and append it to X_train_nb_vec and X_test_nb_vec\n",
    "        # X_train_nb = X_train_nb.to_frame()\n",
    "        # X_test_nb = X_test_nb.to_frame()\n",
    "\n",
    "        # print(type(X_train_nb_vec))\n",
    "        preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "        result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        result['moral_score'] = result.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        result['moral_score'] = result['moral_score'].fillna(-1)\n",
    "        result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        X_train_nb_vec = hstack([X_train_nb_vec, result_moral_matrix])\n",
    "        \n",
    "        preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "        result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        #normalizing moral score (scale of 0-10, normalized to 0-1)\n",
    "        result_val['moral_score'] = result_val.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
    "        result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        X_val_nb_vec = hstack([X_val_nb_vec, result_moral_matrix_val])\n",
    "\n",
    "        # # Dropping rows with null value for moral score\n",
    "        keep_indices_train = result[result['moral_score'] != -1].index\n",
    "        keep_indices_val = result_val[result_val['moral_score'] != -1].index\n",
    "        # Filter the stacked matrices to keep only the desired rows\n",
    "        X_train_nb_vec = X_train_nb_vec[keep_indices_train, :]\n",
    "        X_val_nb_vec = X_val_nb_vec[keep_indices_val, :]\n",
    "        y_train_nb = y_train_nb.iloc[keep_indices_train]\n",
    "        y_val_nb = y_val_nb.iloc[keep_indices_val]\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "        # # Normalize the training and validation vectors\n",
    "        # normalizer = Normalizer()\n",
    "        # X_train_nb_vec_normalized = normalizer.fit_transform(X_train_nb_vec)\n",
    "        # X_val_nb_vec_normalized = normalizer.transform(X_val_nb_vec)\n",
    "\n",
    "        # print(X_train_nb_vec_normalized[0])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = LogisticRegression()\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "    \n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "        \n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5646b28-6ce1-4c30-8fd1-f42e0b80ceba",
   "metadata": {},
   "source": [
    "##### Without additional handcrafted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "01db2db7-2f2e-43be-86b9-6d4d26797d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_tfidf:\n",
      "Average Cross-Validation F1 Score: 0.576\n",
      "\n",
      "spacy_sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for spacy_sentences:\n",
      "Average Cross-Validation F1 Score: 0.527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huikhangkiat/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_lr = df_train['content'].copy()\n",
    "y_lr = df_train['label'].copy()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "n_splits = 2  # Or however many folds you want\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define preprocessing and vectorization configurations\n",
    "preprocessing_type = {\n",
    "    # 'lemmatization_tfidf': (preprocess_pipeline_lemma, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True)),\n",
    "    # 'stemming_tfidf': (preprocess_pipeline_stem, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'vanilla_tfidf': (None, TfidfVectorizer(ngram_range=(1,1), sublinear_tf=True)),\n",
    "    'spacy_tfidf': (preprocess_spacy, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)),\n",
    "    # 'lemmatization_bow': (preprocess_pipeline_lemma, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'stemming_bow': (preprocess_pipeline_stem, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    # 'vanilla_bow': (None, CountVectorizer(ngram_range=(1,1))),\n",
    "    # 'spacy_bow': (preprocess_spacy, CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x: x)),\n",
    "    'spacy_sentences': (preprocess_spacy_moral_sentences, TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,))\n",
    "\n",
    "}\n",
    "\n",
    "for preprocessor, (preprocess_pipeline, vectorizer) in preprocessing_type.items():\n",
    "    val_f1s = []\n",
    "    print(preprocessor)\n",
    "    for train_index, test_index in skf.split(X_nb, y_nb):\n",
    "        X_train_nb, X_val_nb = X_nb[train_index], X_nb[test_index]\n",
    "        y_train_nb, y_val_nb = y_nb[train_index], y_nb[test_index]\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if preprocess_pipeline:\n",
    "            X_train_nb_preprocessed = preprocess_pipeline(X_train_nb)\n",
    "            X_val_nb_preprocessed = preprocess_pipeline(X_val_nb)\n",
    "        # Vectorize data\n",
    "        X_train_nb_vec = vectorizer.fit_transform(X_train_nb_preprocessed)\n",
    "        X_val_nb_vec = vectorizer.transform(X_val_nb_preprocessed)\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "\n",
    "        if preprocess_pipeline == preprocess_spacy_moral_sentences:\n",
    "            y_train_nb = np.repeat(y_train_nb, 5)\n",
    "            y_val_nb = np.repeat(y_val_nb, 5)\n",
    "            # print(y_train_nb.shape)\n",
    "            # print(y_train_nb[0:10])\n",
    "        \n",
    "        \n",
    "        # # Add morality score and append it to X_train_nb_vec and X_test_nb_vec\n",
    "        # X_train_nb = X_train_nb.to_frame()\n",
    "        # X_test_nb = X_test_nb.to_frame()\n",
    "\n",
    "        # # print(type(X_train_nb_vec))\n",
    "        # preprocessed_spacy_train = preprocess_pipeline(X_train_nb, moral='Yes')\n",
    "        # result = moralstrength.estimate_morals(preprocessed_spacy_train, process=False) # set to false if text is alredy pre-processed\n",
    "        # result['moral_score'] = result.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        # result['moral_score'] = result['moral_score'].fillna(-1)\n",
    "        # result_moral_matrix = csr_matrix((result['moral_score']).values.reshape(-1, 1)) # Reshape if necessary\n",
    "        # X_train_nb_vec = hstack([X_train_nb_vec, result_moral_matrix])\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        \n",
    "        # preprocessed_spacy_val = preprocess_pipeline(X_val_nb, moral='Yes')\n",
    "        # result_val = moralstrength.estimate_morals(preprocessed_spacy_val, process=False) # set to false if text is alredy pre-processed\n",
    "        # #normalizing moral score (scale of 0-10, normalized to 0-1)\n",
    "        # result_val['moral_score'] = result_val.mean(axis=1)/10 #normalize score to 0 to 1\n",
    "        # result_val['moral_score'] = result_val['moral_score'].fillna(-1) #assign moral score of -1 to null values\n",
    "        # result_moral_matrix_val = csr_matrix((result_val['moral_score']).values.reshape(-1, 1))\n",
    "        # X_val_nb_vec = hstack([X_val_nb_vec, result_moral_matrix_val])\n",
    "\n",
    "        # # # Dropping rows with null value for moral score\n",
    "        # keep_indices_train = result[result['moral_score'] != -1].index\n",
    "        # keep_indices_val = result_val[result_val['moral_score'] != -1].index\n",
    "        # # Filter the stacked matrices to keep only the desired rows\n",
    "        # X_train_nb_vec = X_train_nb_vec[keep_indices_train, :]\n",
    "        # X_val_nb_vec = X_val_nb_vec[keep_indices_val, :]\n",
    "        # y_train_nb = y_train_nb.iloc[keep_indices_train]\n",
    "        # y_val_nb = y_val_nb.iloc[keep_indices_val]\n",
    "\n",
    "        # print(X_train_nb_vec.shape)\n",
    "        # print(X_train_nb_vec[0:10])\n",
    "        # print(y_train_nb.shape)\n",
    "        # print(y_train_nb[0:10])\n",
    "        # # # Normalize the training and validation vectors\n",
    "        # # normalizer = Normalizer()\n",
    "        # # X_train_nb_vec_normalized = normalizer.fit_transform(X_train_nb_vec)\n",
    "        # # X_val_nb_vec_normalized = normalizer.transform(X_val_nb_vec)\n",
    "\n",
    "        # # print(X_train_nb_vec_normalized[0])\n",
    "\n",
    "        # Initialize the Bernoulli Naive Bayes model\n",
    "        model = LogisticRegression()\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_nb_vec, y_train_nb)\n",
    "    \n",
    "        # Predict on the test set\n",
    "        y_pred_nb = model.predict(X_val_nb_vec)\n",
    "        \n",
    "        val_f1s.append(f1_score(y_val_nb, y_pred_nb))\n",
    "\n",
    "    # Calculate and print the average F1 score across all validation folds\n",
    "    print(f\"Results for {preprocessor}:\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {np.mean(val_f1s):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae092424-1ae3-432e-b2ec-f132dbfc21da",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429788c-5f4d-40cd-98c3-2d6d829bb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR_final = df_train['content'].copy()\n",
    "y_train_LR_final = df_train['label'].copy()\n",
    "X_test_LR_final = df_test['content'].copy()\n",
    "y_test_LR_final = df_test['label'].copy()\n",
    "\n",
    "#using spacy_tfidf as vectorizer \n",
    "prepocess_pipeline = preprocess_spacy\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, sublinear_tf=True,)\n",
    "\n",
    "#preprocess\n",
    "X_train_LR_final = preprocess_pipeline(X_train_LR_final)\n",
    "X_test_LR_final = preprocess_pipeline(X_test_LR_final)\n",
    "\n",
    "print(X_train_LR_final.shape)\n",
    "# Vectorize data\n",
    "X_train_LR_final_vec = vectorizer.fit_transform(X_train_LR_final)\n",
    "X_test_LR_final_vec = vectorizer.transform(X_test_LR_final)\n",
    "print(X_train_LR_final_vec.shape)\n",
    "\n",
    "model_LR_final = LogisticRegression()\n",
    "model_LR_final.fit(X_train_LR_final_vec, y_train_LR_final)\n",
    "\n",
    "y_pred_LR_final = model_LR_final.predict(X_test_LR_final_vec)\n",
    "test_f1_score_lr = f1_score(y_test_LR_final, y_pred_LR_final)\n",
    "\n",
    "print(f\"Final Test F1 Score for Logistic Regression: {test_f1_score_lr:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cfd30-309b-43af-be5d-7817b420d8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
